{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Learning, Distance and (k)NN method\n",
    "\n",
    "> Weitong Zhang\n",
    "> 2015011493\n",
    ">\n",
    "> <zwt15@mails.tsinghua.edu.cn>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voronor Gird in Euclid Space\n",
    "\n",
    "We are about to prove the following statement:\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\forall i\\ \\|\\vec x_i - \\vec s_1 \\|_2^2 \\ge \\|\\vec x_0 - \\vec s_1 \\|_2^2\\\\\n",
    "\\forall i\\ \\|\\vec x_i - \\vec s_2 \\|_2^2 \\ge \\|\\vec x_0 - \\vec s_2 \\|_2^2\\\\\n",
    "\\forall t\\ \\in [0,1], \\vec s = t\\vec s_1 + (1-t) \\vec s_2\n",
    "\\end{cases} \\Rightarrow \\forall i\\ \\|\\vec x_i - \\vec s \\|_2^2 \\ge \\|\\vec x_0 - \\vec s \\|_2^2\n",
    "$$\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\|\\vec x_i - \\vec s \\|_2^2 - \\|\\vec x_0 - \\vec s \\|_2^2 = \\sum_j (x_{ij} - s_j)^2 - \\sum_j (x_{0j} - s_j)^2 = \\sum_j (x_{ij} - x_{0j})(x_{ij} + x_{0j} - 2s_j)\\\\\n",
    "\\|\\vec x_i - \\vec s_1 \\|_2^2 - \\|\\vec x_0 - \\vec s_1 \\|_2^2 = \\sum_j (x_{ij} - s_{1j})^2 - \\sum_j (x_{0j} - s_{1j})^2 = \\sum_j (x_{ij} - x_{0j})(x_{ij} + x_{0j} - 2s_{1j}) \\ge 0\\\\\n",
    "\\|\\vec x_i - \\vec s_2 \\|_2^2 - \\|\\vec x_0 - \\vec s_2 \\|_2^2 = \\sum_j (x_{ij} - s_{2j})^2 - \\sum_j (x_{0j} - s_{2j})^2 = \\sum_j (x_{ij} - x_{0j})(x_{ij} + x_{0j} - 2s_{2j}) \\ge 0\n",
    "\\end{cases}$$\n",
    "\n",
    "We can easily found out that the first inequation is the linear combination of the later two, therefore, \n",
    "\n",
    "$$ \\begin{aligned}\n",
    "\\|\\vec x_i - \\vec s \\|_2^2 - \\|\\vec x_0 - \\vec s \\|_2^2 = \\sum_j (x_{ij} - s_j)^2 - \\sum_j (x_{0j} - s_j)^2 = \\sum_j (x_{ij} - x_{0j})(x_{ij} + x_{0j} - 2s_j) \\\\\n",
    "= t(\\sum_j (x_{ij} - x_{0j})(x_{ij} + x_{0j} - 2s_{1j})) + (1-t) (\\sum_j (x_{ij} - x_{0j})(x_{ij} + x_{0j} - 2s_{2j})) \\ge 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "Therefore, the set determined by Voronoi is a convex set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error rate of NN method\n",
    "\n",
    "### Error rate of Baysian Method\n",
    "\n",
    "Since the probability of $x \\in [0,\\frac{cr}{c-1}]$, the classifier could be only randomly chosen from all of the $c$ categories\n",
    "\n",
    "$$P^* = \\sum_i P(w_i)P_{err} = \\frac{cr}{c-1} \\times \\frac{c-1}{c} = r$$\n",
    "\n",
    "### Error rate of NN method\n",
    "\n",
    "Suppose that the number of samples in the training dataset is sufficient, i.e. for each $x$ where $p(x|w_i) \\ne 0$, there are enough samples belong to $w_i$\n",
    "\n",
    "For each $x\\in w_i$, if $x \\in [i,i+ 1 - \\frac{cr}{c-1}]$, the NN method will not generate error, if $x\\in [0,\\frac{cr}{c-1}]$, the nearest sample of $x$ belongs to all of the $c$ categories, therefore, the probability of error is $\\frac{c-1}{c}$\n",
    "\n",
    "Therefore, the error rate of NN method is \n",
    "\n",
    "$$\\frac{cr}{c-1} \\times \\frac{c-1}{c} = r = P^*$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minkowski Distance\n",
    "\n",
    "Minkowski distance should be described as:\n",
    "\n",
    "$$ D(X,Y) = (\\sum_i |x_i - y_i|^p )^{1/p}$$\n",
    "\n",
    "According to the definition of distance, a distance should obey the following rules:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "D(p,q) & \\ge 0, D(p,q) = 0 \\Leftrightarrow p = q\\\\\n",
    "D(p,q) &= D(q,p)\\\\\n",
    "D(p,q) &\\le D(p,z) + D(q,z)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Owing to the fact that $|x| = |-x| \\ge 0, |x| = 0 \\Leftrightarrow x = 0$, the first two rules are easily satisfied. We are about to prove:\n",
    "\n",
    "$$ (\\sum_i |x_i + y_i|^p )^{1/p} \\le (\\sum_i |x_i|^p )^{1/p} + (\\sum_i |y_i|^p )^{1/p}$$\n",
    "\n",
    "$$ \\sum_i |x_i + y_i|^p = \\sum_i |x_i + y_i| \\times |x_i + y_i|^{p-1} \\le \\sum_i |x_i| \\times |x_i + y_i|^{p-1} + \\sum_i |y_i| \\times |x_i + y_i|^{p-1}$$\n",
    "\n",
    "Now, we have to use the $\\mathrm {H\\ddot older}$ inequation:\n",
    "\n",
    "$$\\sum_i u_iv_i \\le (\\sum u_i^p)^{\\frac1p}(v_i^q)^{\\frac1q}, \\text{ where } \\frac1p + \\frac1q = 1$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\sum_i |x_i| \\times |x_i + y_i|^{p-1} \\le (\\sum_i |x_i|^p)^{\\frac1p}(\\sum_i|x_i + y_i|^{(p-1)q})^{\\frac1q}, \\text{ where } \\frac1p + \\frac1q = 1 \\Rightarrow pq - q = p$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\sum_i |x_i| \\times |x_i + y_i|^{p-1} + \\sum_i |y_i| \\times |x_i + y_i|^{p-1} \\le (\\sum_i |x_i|^p)^{\\frac1p}(\\sum_i |x_i + y_i|^p)^{\\frac1q} + (\\sum_i |y_i|^p)^{\\frac1p}(\\sum_i |x_i + y_i|^p)^{\\frac1q}$$\n",
    "\n",
    "Therefore, since $\\frac1p + \\frac1q = 1$, we get\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\sum_i |x_i + y_i|^p \\le ((\\sum_i |y_i|^p)^{\\frac1p} + (\\sum_i |x_i|^p)^{\\frac1p})(\\sum_i |x_i + y_i|^p)^{\\frac1q}\\\\\n",
    "&\\Leftrightarrow \\frac{\\sum_i |x_i + y_i|^p}{(\\sum_i |x_i + y_i|^p)^{\\frac1q}} \\le (\\sum_i |y_i|^p)^{\\frac1p} + (\\sum_i |x_i|^p)^{\\frac1p}\\\\\n",
    "&\\Leftrightarrow (\\sum_i |x_i + y_i|^p )^{1/p} \\le (\\sum_i |x_i|^p )^{1/p} + (\\sum_i |y_i|^p )^{1/p}\n",
    "\\end{aligned}$$\n",
    "\n",
    "The prove above use the [Hölder's inequality](https://en.wikipedia.org/wiki/Hölder%27s_inequality#Proof_of_Hölder's_inequality)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming\n",
    "\n",
    "### Getting the MNIST data\n",
    "\n",
    "In order to minimize the file to upload, we get the MNIST data from website each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import struct\n",
    "import sys\n",
    "try: \n",
    "    from urllib.request import urlretrieve \n",
    "except ImportError: \n",
    "    from urllib import urlretrieve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "def loadData(src, cimg):\n",
    "    print ('Downloading ' + src)\n",
    "    gzfname, h = urlretrieve(src, './delete.me')\n",
    "    try:\n",
    "        with gzip.open(gzfname) as gz:\n",
    "            n = struct.unpack('I', gz.read(4))\n",
    "            # Read magic number.\n",
    "            if n[0] != 0x3080000:\n",
    "                raise Exception('Invalid file: unexpected magic number.')\n",
    "            # Read number of entries.\n",
    "            n = struct.unpack('>I', gz.read(4))[0]\n",
    "            if n != cimg:\n",
    "                raise Exception('Invalid file: expected {0} entries.'.format(cimg))\n",
    "            crow = struct.unpack('>I', gz.read(4))[0]\n",
    "            ccol = struct.unpack('>I', gz.read(4))[0]\n",
    "            if crow != 28 or ccol != 28:\n",
    "                raise Exception('Invalid file: expected 28 rows/cols per image.')\n",
    "            # Read data.\n",
    "            res = np.frombuffer(gz.read(cimg * crow * ccol), dtype = np.uint8)\n",
    "    finally:\n",
    "        os.remove(gzfname)\n",
    "    return res.reshape((cimg, crow * ccol))\n",
    "\n",
    "def loadLabels(src, cimg):\n",
    "    print ('Downloading ' + src)\n",
    "    gzfname, h = urlretrieve(src, './delete.me')\n",
    "    try:\n",
    "        with gzip.open(gzfname) as gz:\n",
    "            n = struct.unpack('I', gz.read(4))\n",
    "            # Read magic number.\n",
    "            if n[0] != 0x1080000:\n",
    "                raise Exception('Invalid file: unexpected magic number.')\n",
    "            # Read number of entries.\n",
    "            n = struct.unpack('>I', gz.read(4))\n",
    "            if n[0] != cimg:\n",
    "                raise Exception('Invalid file: expected {0} rows.'.format(cimg))\n",
    "            # Read labels.\n",
    "            res = np.frombuffer(gz.read(cimg), dtype = np.uint8)\n",
    "    finally:\n",
    "        os.remove(gzfname)\n",
    "    return res.reshape((cimg, 1))\n",
    "\n",
    "def try_download(dataSrc, labelsSrc, cimg):\n",
    "    data = loadData(dataSrc, cimg)\n",
    "    labels = loadLabels(labelsSrc, cimg)\n",
    "    return data.astype(np.float32)/256.0,labels\n",
    "\n",
    "# URLs for the train image and label data\n",
    "url_train_image = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
    "url_train_labels = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
    "num_train_samples = 60000\n",
    "train_data,train_labels = try_download(url_train_image, url_train_labels, num_train_samples)\n",
    "\n",
    "# URLs for the test image and label data\n",
    "url_test_image = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\n",
    "url_test_labels = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "num_test_samples = 10000\n",
    "test_data,test_labels = try_download(url_test_image, url_test_labels, num_test_samples)\n",
    "\n",
    "train_sum = 60000\n",
    "test_sum = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel of NN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_nn(size=60000,k=5,p=2):\n",
    "    data_used, _, label_used, _ = train_test_split(train_data,train_labels,test_size = train_sum - size)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k,p=p,n_jobs=-1)\n",
    "    start = time.clock()\n",
    "    neigh.fit(data_used,np.ravel(label_used))\n",
    "    predict = neigh.predict(test_data)\n",
    "    end = time.clock()\n",
    "    return accuracy_score(test_labels,predict),end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The influence of number of samples on the performance of the classifier\n",
    "\n",
    "According to the algorithm, the space complexity is $\\mathcal O(n)$ (suppose the sort function could be done locally). The time complexity is $\\mathcal O(N\\log N) + \\mathcal O(Nd)$, where $N$ is the number of samples. $d$ is the dimension of the features.\n",
    "\n",
    "The accuracy of the classifier would keep increasing as the number of the samples increasing. However, the increasing rate would be slow down\n",
    "\n",
    "In this experiment, we set $k = 5$ and the distance function is euclid distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples = 6, Accuracy = 8.9%, TimeElapsed = 7.308370e-01s\n",
      "Samples = 60, Accuracy = 55.6%, TimeElapsed = 1.233036e+00s\n",
      "Samples = 600, Accuracy = 84.3%, TimeElapsed = 1.574072e+01s\n",
      "Samples = 6000, Accuracy = 93.9%, TimeElapsed = 1.957661e+02s\n",
      "Samples = 60000, Accuracy = 96.9%, TimeElapsed = 1.621419e+03s\n"
     ]
    }
   ],
   "source": [
    "for samples in [6,60,600,6000,60000]:\n",
    "    p,t = main_nn(size=samples)\n",
    "    print('Samples = {}, Accuracy = {:.1f}%, TimeElapsed = {:e}s'.format(samples,p * 100,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The influence of $k$ on the performance of the classifier\n",
    "\n",
    "We test $k = 1,3,5,7,9,11$ in the situation that number of samples is 6,000 to check the incluence of $k$ on the performance of the classifier. The distance function is still set to euclid function.\n",
    "\n",
    "As the result shows, the accuracy of the performance is not increasing, This might because the number of samples is too large, therefore, we do this experiment again setting the number of samples to 600.\n",
    "\n",
    "After setting the number of samples to 600, we can find out that the accuracy of the classifier is dropping, which might because the train set is too small, that the 11-th sample, 9-th sample are not belong to the same category of the testing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 6,000 samples: \n",
      "k = 1, Accuracy = 93.9%, TimeElapsed = 1.826142e+02s\n",
      "k = 3, Accuracy = 94.0%, TimeElapsed = 2.153683e+02s\n",
      "k = 5, Accuracy = 93.8%, TimeElapsed = 1.743024e+02s\n",
      "k = 7, Accuracy = 93.8%, TimeElapsed = 1.643828e+02s\n",
      "k = 9, Accuracy = 93.4%, TimeElapsed = 1.675853e+02s\n",
      "k = 11, Accuracy = 93.0%, TimeElapsed = 2.062031e+02s\n",
      "In 600 samples: \n",
      "k = 1, Accuracy = 87.0%, TimeElapsed = 2.327630e+01s\n",
      "k = 3, Accuracy = 83.8%, TimeElapsed = 1.876983e+01s\n",
      "k = 5, Accuracy = 84.8%, TimeElapsed = 1.583720e+01s\n",
      "k = 7, Accuracy = 83.0%, TimeElapsed = 1.719769e+01s\n",
      "k = 9, Accuracy = 84.4%, TimeElapsed = 1.760499e+01s\n",
      "k = 11, Accuracy = 81.1%, TimeElapsed = 2.172389e+01s\n"
     ]
    }
   ],
   "source": [
    "print('In 6,000 samples: ')\n",
    "for k in [1,3,5,7,9,11]:\n",
    "    p,t = main_nn(size=6000,k=k)\n",
    "    print('k = {}, Accuracy = {:.1f}%, TimeElapsed = {:e}s'.format(k,p * 100,t))\n",
    "print('In 600 samples: ')    \n",
    "for k in [1,3,5,7,9,11]:\n",
    "    p,t = main_nn(size=600, k=k)\n",
    "    print('k = {}, Accuracy = {:.1f}%, TimeElapsed = {:e}s'.format(k,p * 100,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using different distance function\n",
    "\n",
    "We use minkowski distance, where $p = 1$ (Manhattan Distance), $p = 2$ (Euclid Distance, mentioned above),$p = \\infty$ (Chebyshev distance) to carry on this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 1, Accuracy = 96.2%, TimeElapsed = 1.395925e+03s\n",
      "p = 2, Accuracy = 96.9%, TimeElapsed = 1.491500e+03s\n",
      "p = inf, Accuracy = 82.0%, TimeElapsed = 1.211863e+03s\n"
     ]
    }
   ],
   "source": [
    "for s in [1,2,float('inf')]:\n",
    "    p,t = main_nn(p=s)\n",
    "    print('p = {}, Accuracy = {:.1f}%, TimeElapsed = {:e}s'.format(s,p * 100,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the weight for each element\n",
    "\n",
    "Our idea is to split the training dataset to validation dataset and training dataset, and use the validation dataset to correct the $\\vec a$, all of the algorithm is described below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch 0: accuracy: 93.5%, max A = 0.5055099927808507, min A = 0.49996787256900094\n",
      "Minibatch 10: accuracy: 93.5%, max A = 0.559354316315876, min A = 0.49990211677973995\n",
      "Minibatch 20: accuracy: 93.4%, max A = 0.6108741201380742, min A = 0.4998424197889787\n",
      "Minibatch 30: accuracy: 93.9%, max A = 0.6573919243102042, min A = 0.4999162005120227\n",
      "Minibatch 40: accuracy: 93.5%, max A = 0.7027156033852391, min A = 0.49989666787345455\n",
      "Minibatch 50: accuracy: 94.1%, max A = 0.7447137072905489, min A = 0.4998838176501185\n",
      "Minibatch 60: accuracy: 94.1%, max A = 0.7798801837313025, min A = 0.49985455884878827\n",
      "Minibatch 70: accuracy: 94.0%, max A = 0.812561777096363, min A = 0.49974980951606035\n",
      "Minibatch 80: accuracy: 93.5%, max A = 0.8404695532318701, min A = 0.49959202563092187\n",
      "Minibatch 90: accuracy: 94.5%, max A = 0.864383612005931, min A = 0.4995014912335734\n"
     ]
    }
   ],
   "source": [
    "def get_A():\n",
    "    A = np.zeros(28 * 28)\n",
    "    lr = 0.001\n",
    "    epoch = 100\n",
    "    batch_size = 5000\n",
    "    inner_iter = 10\n",
    "    for batch in range(epoch):\n",
    "        td,vd,tl,vl = train_test_split(train_data,train_labels,train_size = batch_size, test_size = batch_size)\n",
    "        for inner in range(inner_iter):\n",
    "            neigh = KNeighborsClassifier(n_neighbors=1,n_jobs=12)\n",
    "            eA = 1.0 / (1 + np.exp(-A))\n",
    "            neigh.fit(td * eA,np.ravel(tl))\n",
    "            pre = neigh.kneighbors(vd * eA)[1]\n",
    "            pre_label = tl[pre]\n",
    "            nei = td[pre]\n",
    "            acc = 0\n",
    "            for i in range(batch_size):\n",
    "                delta = nei[i] - vd[i]\n",
    "                l = np.abs(delta) / np.linalg.norm(delta) / np.linalg.norm(delta)\n",
    "                if pre_label[i] != vl[i]: \n",
    "                    A = A + lr * l \n",
    "                else:\n",
    "                    acc = acc + 1\n",
    "                    A = A - 1e-2 * lr * l\n",
    "        if batch % 10 == 0:\n",
    "            print ('Minibatch {}: accuracy: {:.1f}%, max A = {}, min A = {}'\n",
    "                   .format(batch,acc / batch_size * 100, np.max(eA),np.min(eA)))\n",
    "            \n",
    "    eA = 1.0 / (1 + np.exp(-A))\n",
    "    return eA\n",
    "A = get_A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New: Accuracy = 94.2%, TimeElapsed = 2.059350e+02s\tOld: Accuracy = 93.9%, TimeElapsed = 3.098662e+02s\n",
      "New: Accuracy = 94.2%, TimeElapsed = 1.638133e+02s\tOld: Accuracy = 93.8%, TimeElapsed = 3.183988e+02s\n",
      "New: Accuracy = 94.1%, TimeElapsed = 1.642004e+02s\tOld: Accuracy = 93.9%, TimeElapsed = 3.015391e+02s\n",
      "New: Accuracy = 94.4%, TimeElapsed = 1.648305e+02s\tOld: Accuracy = 93.9%, TimeElapsed = 3.045074e+02s\n",
      "New: Accuracy = 94.0%, TimeElapsed = 1.647256e+02s\tOld: Accuracy = 93.6%, TimeElapsed = 3.128420e+02s\n",
      "New: Accuracy = 94.3%, TimeElapsed = 1.652688e+02s\tOld: Accuracy = 94.1%, TimeElapsed = 3.146009e+02s\n",
      "New: Accuracy = 94.2%, TimeElapsed = 1.639273e+02s\tOld: Accuracy = 93.9%, TimeElapsed = 3.484923e+02s\n",
      "New: Accuracy = 94.2%, TimeElapsed = 1.644854e+02s\tOld: Accuracy = 93.9%, TimeElapsed = 3.055928e+02s\n",
      "New: Accuracy = 94.1%, TimeElapsed = 1.635961e+02s\tOld: Accuracy = 93.6%, TimeElapsed = 1.987871e+02s\n",
      "New: Accuracy = 94.5%, TimeElapsed = 1.977218e+02s\tOld: Accuracy = 94.1%, TimeElapsed = 3.124966e+02s\n"
     ]
    }
   ],
   "source": [
    "def main_nn_with_a(A, size=6000):\n",
    "    data_used, _, label_used, _ = train_test_split(train_data,train_labels,test_size = train_sum - size)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=1,n_jobs=12)\n",
    "    start = time.clock()\n",
    "    neigh.fit(data_used * A,np.ravel(label_used))\n",
    "    predict = neigh.predict(test_data * A)\n",
    "    end = time.clock()\n",
    "    neigh_old = KNeighborsClassifier(n_neighbors=1,n_jobs=-1)\n",
    "    \n",
    "    start_old = time.clock()\n",
    "    neigh_old.fit(data_used,np.ravel(label_used))\n",
    "    predict_old = neigh_old.predict(test_data)\n",
    "    end_old = time.clock()\n",
    "    return accuracy_score(test_labels,predict),end-start, \\\n",
    "        accuracy_score(test_labels,predict_old),end_old-start_old\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    p,t,p_old,t_old = main_nn_with_a(A, size=6000)\n",
    "    print('New: Accuracy = {:.1f}%, TimeElapsed = {:e}s\\tOld: Accuracy = {:.1f}%, TimeElapsed = {:e}s'\n",
    "      .format(p * 100,t,p_old * 100,t_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(A.reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tangent distance on MNIST\n",
    "\n",
    "#### Tangent Distance\n",
    "\n",
    "There are two kinds of tangent distance, which will be discussed as following:\n",
    "\n",
    "- One sided tangent distance\n",
    "\n",
    "Let $S_{\\vec x_p}(\\alpha)$ be a nonlinear mapping from $\\mathbb R^m \\rightarrow\\mathbb R^m$ that deforms\n",
    "the image $\\vec x_p$ by $\\alpha$ where $\\alpha \\in\\mathbb R^s$ is a vector of s deformation operation parameters.\n",
    "\n",
    "$$J = \\frac{\\partial S}{\\partial \\alpha}\\mid_{\\alpha=0}$$\n",
    "\n",
    "Then, the one sided tangent distance should be described as \n",
    "\n",
    "$$D = \\|\\vec y - \\vec x - JJ^{\\mathrm T}(\\vec y - \\vec x)\\|_2$$\n",
    "where $\\vec y$ is a new samples to classify\n",
    "\n",
    "- Two sided tangent distance\n",
    "\n",
    "Suppose that we have got the tangent plane of two samples, \n",
    "\n",
    "$$\\begin{cases}\n",
    "\\text{Training sample: } P + L_P\\alpha_P \\\\\n",
    "\\text{Testing sample: } E + L_E\\alpha_E \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "By minimizing the distance between the two dynamic points, we can find that when \n",
    "\n",
    "$$\\begin{cases}\n",
    "(L_{PE}L_{EE}^{-1}L_E^T - L_P^T)(E-P) = (L_{PE}L_{EE}^{-1}L_{EP} - L_{PP})\\alpha_P \\\\\n",
    "(L_{EP}L_{PP}^{-1}L_P^T - L_E^T)(P-E) = (L_{EP}L_{PP}^{-1}L_{PE} - L_{EE})\\alpha_E \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "The distance arrives at its minimum value\n",
    "\n",
    "$$D = \\|E-P+L_E\\alpha_E -L_P\\alpha_P\\|_2$$\n",
    "\n",
    "It is obvious that the Two sided tangent distance performs better and need more time.\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "In order to make the deformation of the image not too sharp, we introduct the regularization item for two sided tangent distance, This weighting of the tangent distance is particularly useful when some of the\n",
    "tangent vectors of $E$ or $P$ are collinear\n",
    "\n",
    "$$D = \\min \\|E-P+L_E\\alpha_E -L_P\\alpha_P\\|_2^2 + \\lambda (\\|L_E\\alpha_E\\|_2^2 + \\|L_P\\alpha_P\\|_2^2)$$\n",
    "\n",
    "The solution is:\n",
    "\n",
    "$$\\begin{cases}\n",
    "(L_{PE}L_{EE}^{-1}L_E^T - (1+\\lambda)L_P^T)(E-P) = (L_{PE}L_{EE}^{-1}L_{EP} - (1+\\lambda)^2L_{PP})\\alpha_P \\\\\n",
    "(L_{EP}L_{PP}^{-1}L_P^T - (1+\\lambda)L_E^T)(P-E) = (L_{EP}L_{PP}^{-1}L_{PE} - (1+\\lambda)^2L_{EE})\\alpha_E \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    ">NOTICE: To speed up the calculating speed, only 600 samples in training dataset and 600 samples in testing dataset is used, but we will carry on this experiment for 10 times\n",
    "\n",
    "It is obvious that the accuracy for about 75% is much better than the original one 56.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "This chapter is focus on how to solve the function $$D = \\min \\|E-P+L_E\\alpha_E -L_P\\alpha_P\\|_2^2 + \\lambda (\\|L_E\\alpha_E\\|_2^2 + \\|L_P\\alpha_P\\|_2^2)$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\frac{\\partial D}{\\partial \\alpha_E} = 2L_E^T(E-P+L_E\\alpha_E -L_P\\alpha_P) + 2\\lambda L_E^TL_E\\alpha_E = 0\\\\\n",
    "&\\Rightarrow L_E^T(E-P - L_P\\alpha_P) + (1 + \\lambda) L_E^TL_E\\alpha_E = 0\\\\\n",
    "&\\Rightarrow \\frac{\\partial D}{\\partial \\alpha_P} = 0 \\Rightarrow L_P^T(P-E - L_E\\alpha_E) + (1 + \\lambda) L_P^TL_P\\alpha_P = 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "Set $L_{EE} = L_E^TL_E, L_{EP} = L_E^TL_P, L_{PE} = L_P^TL_E, L_{PP} = L_P^TL_P, \\Delta = E - P$, we get\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{L_{EE}^{-1}}{1+\\lambda}(L_E^T\\Delta - L_{EP}\\alpha_P) &= \\alpha_E\\\\\n",
    "-L_P^T\\Delta - \\frac{L_{PE}L_{EE}^{-1}}{1+\\lambda}(L_E^T\\Delta - L_{EP}\\alpha_P) + (1 + \\lambda)L_{PP}\\alpha_P &= 0\\\\\n",
    "(L_{PE}L_{EE}^{-1}L_{EP} + (1 + \\lambda)^2L_{PP})\\alpha_P &= ((1+\\lambda)L_P^T + {L_{PE}L_{EE}^{-1}L_E^T})\\Delta\n",
    "\\end{aligned}$$\n",
    "\n",
    "Therefore, we get the following result:\n",
    "\n",
    "$$\\begin{cases}\n",
    "(L_{PE}L_{EE}^{-1}L_E^T - (1+\\lambda)L_P^T)(E-P) = (L_{PE}L_{EE}^{-1}L_{EP} - (1+\\lambda)^2L_{PP})\\alpha_P \\\\\n",
    "(L_{EP}L_{PP}^{-1}L_P^T - (1+\\lambda)L_E^T)(P-E) = (L_{EP}L_{PP}^{-1}L_{PE} - (1+\\lambda)^2L_{EE})\\alpha_E \\\\\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def get_J(arr):\n",
    "    deformation_numbers = 10\n",
    "    J = np.zeros((deformation_numbers,28*28))\n",
    "    img = arr.reshape((28,28))\n",
    "    \n",
    "    ## 0: rotation: CW\n",
    "    rot_mat = cv2.getRotationMatrix2D((14,14), 10, 1.0)\n",
    "    res = cv2.warpAffine(img, rot_mat, (28,28), flags=cv2.INTER_LINEAR)\n",
    "    J[0,:] = ((res - img)/10).reshape(28*28)\n",
    "    \n",
    "    ## 1: rotation: CCW: (to avoid the nonlinear features)\n",
    "    rot_mat = cv2.getRotationMatrix2D((14,14), -10, 1.0)\n",
    "    res = cv2.warpAffine(img, rot_mat, (28,28), flags=cv2.INTER_LINEAR)\n",
    "    J[1,:] = ((res - img)/10).reshape(28*28)\n",
    "    \n",
    "    ## 2: Left\n",
    "    res = np.concatenate((img[:, 3:], img[:, :3]), axis=1)\n",
    "    J[2,:] = ((res - img)/3).reshape(28*28)\n",
    "    \n",
    "    ## 3: Right\n",
    "    res = np.concatenate((img[:, 28 - 3:], img[:, :28 - 3]), axis=1)\n",
    "    J[3,:] = ((res - img)/3).reshape(28*28)\n",
    "    \n",
    "    ## 4: Up\n",
    "    res = np.concatenate((img[3:, :], img[:3, :]), axis=0)\n",
    "    J[4,:] = ((res - img)/3).reshape(28*28)\n",
    "    \n",
    "    ## 5: Down\n",
    "    res = np.concatenate((img[28 - 3:, :], img[:28 - 3, :]), axis=0)\n",
    "    J[5,:] = ((res - img)/3).reshape(28*28)\n",
    "    \n",
    "    ## 6: 1.25x scale\n",
    "    res = cv2.resize(img,(34,34),interpolation=cv2.INTER_CUBIC)\n",
    "    res = res[3:31,3:31]\n",
    "    J[6,:] = ((res - img)/(6/28)).reshape(28*28)\n",
    "    \n",
    "    ## 7: 0.8x scale\n",
    "    res = cv2.copyMakeBorder(img,6,6,6,6,cv2.BORDER_REPLICATE)\n",
    "    res = cv2.resize(res,(28,28),interpolation=cv2.INTER_CUBIC)\n",
    "    J[7,:] = ((res - img)/(6/28)).reshape(28*28)\n",
    "    \n",
    "    ## 8: erosion\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(2, 2))  \n",
    "    res = cv2.erode(img,kernel)\n",
    "    J[8,:] = (res - img).reshape(28*28)\n",
    "\n",
    "    ## 9: dilation\n",
    "    res = cv2.dilate(img,kernel)\n",
    "    J[9,:] = (res - img).reshape(28*28)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tangent(x,y):\n",
    "    k = 1\n",
    "    \n",
    "    E = np.matrix(x.reshape(-1,1))\n",
    "    P = np.matrix(y.reshape(-1,1))\n",
    "    L_E = np.matrix(get_J(E)).T\n",
    "    L_P = np.matrix(get_J(P)).T\n",
    "    \n",
    "    L_EE = L_E.T * L_E\n",
    "    L_EP = L_E.T * L_P\n",
    "    L_PE = L_P.T * L_E\n",
    "    L_PP = L_P.T * L_P\n",
    "    L_EE_inv = L_EE.I\n",
    "    L_PP_inv = L_PP.I\n",
    "    E_P = E - P\n",
    "    \n",
    "    alpha_P = np.linalg.inv(L_PE * L_EE_inv * L_EP - (1 + k)* (1 + k) * L_PP) * \\\n",
    "        (L_PE * L_EE_inv * L_E.T - (1 + k) * L_P.T) * E_P\n",
    "    alpha_E = np.linalg.inv(L_EP * L_PP_inv * L_PE - (1 + k)* (1 + k) * L_EE) * \\\n",
    "        (L_EP * L_PP_inv * L_P.T - (1 + k) * L_E.T) *  - E_P\n",
    "    \n",
    "    l1 = L_E * alpha_E\n",
    "    l2= L_P * alpha_P\n",
    "    \n",
    "    D = np.linalg.norm(E_P + l1 -l2)**2 + k * (np.linalg.norm(l1)**2 + np.linalg.norm(l2)**2)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.685, Var Accuracy: 0.004024999999999999\n"
     ]
    }
   ],
   "source": [
    "m = []\n",
    "for _ in range(10):\n",
    "    size = 60\n",
    "    test_size = 60\n",
    "    data_used, _, label_used, _ = train_test_split(train_data,train_labels,test_size = train_sum - size)\n",
    "    _, test_used, _, test_label_used = train_test_split(test_data,test_labels,test_size = test_size)\n",
    "\n",
    "    neigh = KNeighborsClassifier(n_neighbors=1,metric=tangent,n_jobs=-1)\n",
    "    neigh.fit(data_used,np.ravel(label_used))\n",
    "    predict = neigh.predict(test_used)\n",
    "    m.append(accuracy_score(test_label_used,predict))\n",
    "print ('Mean Accuracy: {}, Var Accuracy: {}'.format(np.mean(m),np.var(m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice: The code above has to run for hours to get the final result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
