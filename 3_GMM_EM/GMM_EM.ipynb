{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model and Expectation Maximization Algorithm\n",
    "\n",
    "> Weitong Zhang\n",
    "> 2015011493\n",
    ">\n",
    "> <zwt15@mails.tsinghua.edu.cn>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM and Gradient Descent\n",
    "\n",
    "Setting $\\sigma^2 = \\beta$, we get \n",
    "\n",
    "$$P(x_i|\\mu_k,\\beta_kI,x_i \\in \\omega_k) = \\frac1{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x-\\mu_k\\|_2^2}{\\beta_k})$$\n",
    "\n",
    "### Calc $\\mu$\n",
    "\n",
    "#### EM method on $\\mu$\n",
    "\n",
    "$$\\begin{aligned}z_{ik} &= Prob(x_i \\in \\omega_k | x_i,\\mu_k,\\beta_k) = \\frac{P(x_i|\\mu_k,\\beta_k,x_i \\in \\omega_k)P(\\omega_k)}{\\sum_jP(x_i|\\mu_j,\\beta_j,x_i \\in \\omega_j)P(\\omega_j)} \\\\&= \\frac{\\beta_k^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\pi_k}{\\sum_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})\\pi_j}\\end{aligned}$$\n",
    "\n",
    "We want to maximize the following function:\n",
    "\n",
    "$$\\begin{aligned}f(\\mu) &= \\sum_{i=1}^n\\sum_{k=1}^Kz_{ik}[\\ln [\\frac1{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})] + \\ln \\pi_k] \\\\&= \\sum_{i=1}^n\\sum_{k=1}^Kz_{ik}[-\\frac d2\\ln (2\\pi\\beta_k) - 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}  + \\ln \\pi_k]\\end{aligned}$$\n",
    "\n",
    "We have to notice that the $z_{ik}$ is determined by the previous step and should be a constant in this step, therefore, $\\forall i,j,k, \\frac{\\partial z_{ik}}{\\partial\\mu_j} = 0$\n",
    "\n",
    "Therefore, we get:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial f(\\mu)}{\\partial \\mu_k} &= \\frac\\partial{\\partial \\mu_k} \\sum_{i=1}^n\\sum_{k=1}^Kz_{ik}[-\\frac d2\\ln (2\\pi\\beta_k) - 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}  + \\ln \\pi_k] \\\\\n",
    "&= \\frac\\partial{\\partial \\mu_k} \\sum_{i=1}^nz_{ik}[-\\frac d2\\ln (2\\pi\\beta_k) - 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}  + \\ln \\pi_k] \\\\\n",
    "&=\\frac\\partial{\\partial \\mu_k} \\sum_{i=1}^nz_{ik}[- 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}] = 0 \\Leftrightarrow \\frac\\partial{\\partial \\mu_k} \\sum_{i=1}^nz_{ik}[- 0.5\\|x_i-\\mu_k\\|_2^2] = 0 \\\\\n",
    "&\\Rightarrow \\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^nz_{ik}x_i}{\\sum_{i=1}^nz_{ik}} = \n",
    "\\frac{\\sum_{i=1}^n\\frac{x_i\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\pi_k}{\\sum_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})\\pi_j}}{\\sum_{i=1}^n\\frac{\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\pi_k}{\\sum_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})\\pi_j}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "#### Gradient Descent method on $\\mu$\n",
    "\n",
    "$$l(\\mu) = \\sum_{i=1}^n\\ln(\\sum_{k=1}^K\\frac{\\pi_k}{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial \\mu_k} &= \\sum_{i=1}^n\\frac{\\partial}{\\partial \\mu_k}\\ln(\\sum_{k=1}^K\\frac{\\pi_k}{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}) \\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\frac{\\partial}{\\partial \\mu_k} \\pi_k\\beta_k^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{(\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})} \\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})(x_i-\\mu_k)}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}\\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}x_i - \\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}\\mu_k\n",
    "\\end{aligned}$$\n",
    "\n",
    "By setting \n",
    "\n",
    "$$\\eta_k = \\frac1{\\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}} > 0$$\n",
    "\n",
    "We can conclude that \n",
    "\n",
    "$$\\mu_k^{(t+1)} = \\mu_k + \\eta_k\\nabla l(\\mu) = \\frac{\\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}x_i}{\\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}} = \\frac{\\sum_{i=1}^n\\frac{x_i\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\pi_k}{\\sum_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})\\pi_j}}{\\sum_{i=1}^n\\frac{\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\pi_k}{\\sum_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})\\pi_j}}$$\n",
    "\n",
    "Therefore, we can prove that the Gradient Descent method and the EM algorithm is the same with calculating $\\mu$\n",
    "\n",
    "### Calc $\\beta = \\sigma^2$\n",
    "\n",
    "To begin with, let's write down the $f$ and $l$ above:\n",
    "\n",
    "$$\\begin{cases}\n",
    "l(\\beta) = \\sum_{i=1}^n\\ln(\\sum_{k=1}^K\\frac{\\pi_k}{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\\\\n",
    "f(\\beta) = \\sum_{i=1}^n\\sum_{k=1}^Kz_{ik}[-\\frac d2\\ln (2\\pi\\beta_k) - 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}  + \\ln \\pi_k]\n",
    "\\end{cases}$$\n",
    "\n",
    "#### EM method on $\\beta = \\sigma^2$\n",
    "\n",
    "We are about to maximize the function $f(\\beta)$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\frac{\\partial f}{\\partial \\beta_k} = \\sum_{i=1}^nz_{ik}[-\\frac d2\\frac1{\\beta_k} + 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k^2}] = 0, \\beta_k \\ne 0 \\\\\n",
    "&\\Leftrightarrow \\sum_{i=1}^nz_{ik}[-\\frac d2{\\beta_k} + 0.5\\|x_i-\\mu_k\\|_2^2] = 0\\\\\n",
    "&\\Leftrightarrow \\beta_k = \\frac{\\sum_{i=1}^nz_{ik}\\|x_i-\\mu_k\\|_2^2}{d\\sum_{i=1}^nz_{ik}} =\n",
    "\\end{aligned}$$\n",
    "\n",
    "$d$ is the dimension of stochastic variable $x$, and we have to point out the $|\\Sigma|=|\\beta I| = \\beta^d$ in the PDF of Gaussian distribution\n",
    "\n",
    "#### Gradient Descent method on $\\beta$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial f}{\\partial \\beta} &= \\sum_{i=1}^n\\frac\\partial{\\partial \\beta}\\ln(\\sum_{k=1}^K\\frac{\\pi_k}{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\frac\\partial{\\partial \\beta} \\frac{\\pi_k}{\\beta_k^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\frac{\\pi_j}{\\beta_j^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})}\\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})(-\\frac d2\\frac{\\pi_k}{\\beta_k^{\\frac d2 +1}} + 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k^2}\\frac{\\pi_k}{\\beta_k^{\\frac d2}})}{\\sum_{j=1}^K\\frac{\\pi_j}{\\beta_j^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})}\\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\frac{\\pi_k}{\\beta_k^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})(-\\frac d2\\frac{1}{\\beta_k} + 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k^2})}{\\sum_{j=1}^K\\frac{\\pi_j}{\\beta_j^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})}\\\\\n",
    "&=\\sum_{i=1}^nz_{ik}(-\\frac d2\\frac{1}{\\beta_k} + 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k^2})\n",
    "\\end{aligned}$$\n",
    "\n",
    "Let $s_k = \\frac1{\\sum_{i=1}^nz_{ik}\\frac d2}\\beta_k^2$, therefore,\n",
    "\n",
    "$$\\beta_k^{(t+1)} = \\beta_k + s_k\\nabla l(\\beta) =\\frac{\\sum_{i=1}^nz_{ik}0.5\\|x_i-\\mu_k\\|_2^2}{\\sum_{i=1}^nz_{ik}\\frac d2} = \\frac{\\sum_{i=1}^nz_{ik}\\|x_i-\\mu_k\\|_2^2}{d\\sum_{i=1}^nz_{ik}}$$\n",
    "\n",
    "Therefore, we can conclude that the Gradient Descent method and EM algorithm is equivalence in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM for MAP Estimation\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L(\\theta) &= \\sum_x\\ln\\sum_z p(x_i,z_i|\\theta)+\\ln p(\\theta) = \\sum_x \\ln(\\sum_z(q(z_i)\\frac{p(x_i,z_i|\\theta)}{q(z_i)}))+\\ln p(\\theta) \\\\ &\\ge \\sum_x\\sum_zq(z)[\\ln(p(x_i,z_i|\\theta)) -\\ln(q(z))] +\\ln(p(\\theta))\n",
    "\\end{aligned}$$\n",
    "\n",
    "We are about to maximize the follow function \n",
    "\n",
    "$$F(q,\\theta) = \\sum_x\\sum_zq(z)\\ln(p(x_i,z_i|\\theta)) - \\sum_x\\sum_zq(z)\\ln(q(z)) +\\ln(p(\\theta))$$\n",
    "\n",
    "Therefore, we got the E-step is the same with MLE method: $q_{[k+1]}(z) = p(z|x,\\theta_k)$, only in this way we can get\n",
    "\n",
    "$$F(q,\\theta) = \\sum_x\\sum_z p(z|x,\\theta)\\ln\\frac{p(x,z|\\theta)}{p(z|x,\\theta)} + \\ln p(\\theta) = \\sum_x\\sum_z p(z|x,\\theta_k)\\ln p(x|\\theta) + \\ln p(\\theta) = L(\\theta)$$\n",
    "\n",
    "Then we got the M-step to optimize the $\\theta_k$\n",
    "\n",
    "$$\\theta_{[k+1]} = \\mathrm{argmax}F(q,\\theta) = \\mathrm{argmax}_\\theta \\sum_x\\sum_z p(z|x,\\theta_k)\\ln(p(x_i,z_i|\\theta)) + \\ln p(\\theta)$$\n",
    "\n",
    "where $\\theta_k$ is a constant, By assuming that $\\ln p(x, z|\\theta)$ and $\\ln p(\\theta)$ are both concave in $\\theta$, so that the M-step is tractable if it requires only maximizing a linear combination of these quantities.\n",
    "\n",
    "The optimization method should be different according to the different function $\\ln p(x, z|\\theta)$ and $\\ln p(\\theta)$, now it is time to prove that the likelihood function is monotonically increasing with each iteration:\n",
    "\n",
    "From the E-step and M-step and according to the $l \\ge F$ inequality, we got\n",
    "$$l(\\theta_{[k+1]}) \\ge F(q_{[k]},\\theta_{[k+1]}) \\ge F(q_{[k]},\\theta_{[k]}) = l(\\theta_{[k]})$$\n",
    "\n",
    "from which we can conclude that the likelihood function is monotonically increasing with each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming\n",
    "\n",
    "### Data preparing\n",
    "\n",
    "We set the visible $x$ in category $\\omega_1$ as $X_v$, while the ones which $x_3$ is not accessible is $X_h$. So is the $y$ for category $\\omega_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.42, -0.087, 0.58, -0.4, 0.58, 0.089)\n",
      "(-0.2, -3.3, -3.4, -0.31, 0.27, -0.04)\n",
      "(1.3, -0.32, 1.7, 0.38, 0.055, -0.035)\n",
      "(0.39, 0.71, 0.23, -0.15, 0.53, 0.011)\n",
      "(-1.6, -5.3, -0.15, -0.35, 0.47, 0.034)\n",
      "(-0.029, 0.89, -4.7, 0.17, 0.69, 0.1)\n",
      "(-0.23, 1.9, 2.2, -0.011, 0.55, -0.18)\n",
      "(0.27, -0.3, -0.87, -0.27, 0.61, 0.12)\n",
      "(-1.9, 0.76, -2.1, -0.065, 0.49, 0.0012)\n",
      "(0.87, -1.0, -2.6, -0.12, 0.54, -0.063)\n"
     ]
    }
   ],
   "source": [
    "Xv = [[0.42, 1.3, -1.6, -0.23, -1.9], [-0.087, -0.32, -5.3, 1.9, 0.76], [0.58, 1.7, -0.15, 2.2, -2.1]]\n",
    "Xh = [[-0.2, 0.39,-0.029, 0.27, 0.87], [-3.3, 0.71, 0.89, -0.3, -1.0], [-3.4, 0.23, -4.7, -0.87, -2.6]]\n",
    "Yv = [[-0.4, 0.38, -0.35, -0.011, -0.065], [0.58, 0.055, 0.47, 0.55, 0.49], [0.089,-0.035, 0.034, -0.18, 0.0012]]\n",
    "Yh = [[-0.31, -0.15, 0.17, -0.27, -0.12], [0.27, 0.53, 0.69, 0.61, 0.54], [-0.04, 0.011, 0.1, 0.12, -0.063]]\n",
    "for i in range(5):\n",
    "    print(Xv[0][i],Xv[1][i],Xv[2][i],Yv[0][i],Yv[1][i],Yv[2][i])\n",
    "    print(Xh[0][i],Xh[1][i],Xh[2][i],Yh[0][i],Yh[1][i],Yh[2][i])\n",
    "X = zip(*(zip(*Xv) + zip(*Xh)))\n",
    "Y = zip(*(zip(*Yv) + zip(*Yh)))\n",
    "N = 10\n",
    "Nh = Nv = 5\n",
    "var = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Distribution\n",
    "\n",
    "#### Baseline: NO missing data\n",
    "\n",
    "Suppose the PDF function should be described as $\\frac1{(2\\pi)^{1.5}|\\Sigma|^{0.5}}\\exp(-0.5 (x-\\mu)^T\\Sigma^{-1}(x-\\mu))$, the likelihood function should be:\n",
    "\n",
    "$$l(\\theta) = \\sum_x -0.5\\ln(|\\Sigma|) - 0.5(x-\\mu)^T\\Sigma^{-1}(x-\\mu) + \\mathrm{constant}$$\n",
    "\n",
    "Therefore, we want to minimize the following function:\n",
    "\n",
    "$$f(\\theta) = \\sum_x \\ln(|\\Sigma|) + (x-\\mu)^T\\Sigma^{-1}(x-\\mu)$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial \\mu} = \\sum_x (\\Sigma^{-1} + \\Sigma^{-1^T})(\\mu-x) = 0 \\Rightarrow \\mu = \\frac1N \\sum x$$\n",
    "$$\\frac{\\partial f}{\\partial \\Sigma} = 0 \\Rightarrow \\Sigma = \\frac1N \\sum (x-\\bar{x}) (x-\\bar{x})^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu = [-0.0709 -0.6047 -0.911 ]\n",
      "sigma = \n",
      " [[ 0.90617729  0.56778177  0.3940801 ]\n",
      " [ 0.56778177  4.20071481  0.7337023 ]\n",
      " [ 0.3940801   0.7337023   4.541949  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, Math, Latex\n",
    "x = np.array(X)\n",
    "mu = np.mean(x,axis = 1)\n",
    "sigma = np.mat(np.zeros((var,var)))\n",
    "for i in range(N):\n",
    "    x_std = np.mat(x[:,i] - mu)\n",
    "    sigma += x_std.transpose()* x_std\n",
    "sigma = sigma / N\n",
    "print ('mu = {}'.format(mu))\n",
    "print ('sigma = \\n {}'.format(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM method\n",
    "\n",
    "We want to maximize the likelihood function $l(\\theta)$, we set the latent parameter as $s$\n",
    "\n",
    "$$l(\\theta) = \\sum_{x_v}\\ln(P(x_v|\\theta)) + \\sum_{x_h}\\ln(P(x_h|\\theta)) = \\sum_{x_v}\\ln(P(x_v|\\theta)) + \\sum_{x_h}\\ln(\\sum_s P(x_h,s|\\theta)) \\ge \\sum_{x_v}\\ln(P(x_v|\\theta)) + \\sum_{x_h}\\sum_s q(s)(\\ln P(x_h,s|\\theta) - \\ln q(s))$$\n",
    "\n",
    "We are optimizing:\n",
    "\n",
    "$$F(q,\\theta) = \\sum_{x_v}\\ln(P(x_v|\\theta)) + \\sum_{x_h}\\sum_s q(s)(\\ln P(x_h,s|\\theta) - \\ln q(s))$$\n",
    "\n",
    "E-step: $q_{[k+1]}(s) = P(s|x_h,\\theta_{[k]})$\n",
    "\n",
    "M-step $\\theta_{[k+1]} = \\mathrm{argmax}_\\theta\\ \\  \\sum_{x_h}\\sum_sP(s|x_h,\\theta_{[k]})\\ln P(x_h,s|\\theta) + \\sum_{x_v}\\ln(P(x_v|\\theta)) = \\mathrm{argmax}_\\theta \\ \\ F(\\theta,\\theta_k)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Distribution\n",
    "\n",
    "In normal distribution, we can conclude that the $P(s|x_h,\\theta_k)$ is also a normal distribution\n",
    "\n",
    "$$\\begin{aligned}\n",
    "F(\\theta,\\theta_k) &= \\sum_{x_h}\\int_s p(s) \\ln \\frac{\\exp(-0.5([x_h,s]-\\mu)^T\\Sigma^{-1}([x_h,s]-\\mu))}{(2\\pi)^{\\frac d2}|\\Sigma|^{\\frac12}} + \\sum_{x_v} \\ln \\frac{\\exp(-0.5(x_v-\\mu)^T\\Sigma^{-1}(x_v-\\mu))}{(2\\pi)^{\\frac d2}|\\Sigma|^{\\frac12}} \\\\\n",
    "&= \\sum_{x_h}\\int_s p(s) {(-0.5([x_h,s]-\\mu)^T\\Sigma^{-1}([x_h,s]-\\mu))} - 0.5ln{|\\Sigma|} + \\sum_{x_v}  {(-0.5(x_v-\\mu)^T\\Sigma^{-1}(x_v-\\mu))}-0.5\\ln{|\\Sigma|} + Const \\\\\n",
    "&= -0.5 \\{ \\sum_{x_h}\\int_s p(s) ([x_h,s]-\\mu)^T\\Sigma^{-1}([x_h,s]-\\mu) + \\ln|\\Sigma| + \\sum_{x_v} (x_v-\\mu)^T\\Sigma^{-1}(x_v-\\mu) + \\ln |\\Sigma|\\} + Const\n",
    "\\end{aligned}$$\n",
    "\n",
    "Set $G$ as following, then we are about to minimize the $G$\n",
    "\n",
    "$$G(\\theta) = \\sum_{x_h}\\int_s p(s)([x_h,s]-\\mu)^T\\Sigma^{-1}([x_h,s]-\\mu) + \\sum_{x_v} (x_v-\\mu)^T\\Sigma^{-1}(x_v-\\mu) + N\\ln|\\Sigma|$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial G}{\\partial \\mu} &= \\sum_{x_h}\\int_s p(s)2\\Sigma^{-1}(\\mu-[x_h,s]) + \\sum_{x_v} 2\\Sigma^{-1}(\\mu - x_v) = 0\\\\\n",
    "&\\Leftrightarrow \\sum_{x_h}\\int_s p(s)(\\mu-[x_h,s]) + \\sum_{x_v}(\\mu - x_v) = 0 \\\\\n",
    "&\\Leftrightarrow \\sum_{x_h}\\mu - [x_h,E(s)] + \\sum_{x_v}(\\mu -x_v) = 0\\\\\n",
    "&\\Leftrightarrow \\mu = \\frac{\\sum_{x_h}[x_h,E(s)] + \\sum_{x_v} x_v}{N}\n",
    "\\end{aligned}$$\n",
    "\n",
    "$$\\text{set} \\ \\ \\Sigma = \\begin{pmatrix} \\sigma^2_{11} & \\sigma^2_{21} & \\sigma^2_{31} \\\\ \\sigma^2_{12} & \\sigma^2_{22} & \\sigma^2_{32} \\\\ \\sigma^2_{13} & \\sigma^2_{23} & \\sigma^2_{33}\\end{pmatrix}, \\text{we get}\\ \\ E(s) = \\mu_3 + \\begin{pmatrix} \\sigma^2_{11} & \\sigma^2_{12} \\end{pmatrix}\\begin{pmatrix}\\sigma^2_{11} & \\sigma^2_{21}\\\\ \\sigma^2_{12} & \\sigma^2_{22}\\end{pmatrix}^{-1}(x_h - \\begin{pmatrix} \\mu_1 \\\\ \\mu_2\\end{pmatrix})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathrm dG = Ntr(\\Sigma^{-1}\\mathrm d\\Sigma) - \\sum_{x_v}(x_v-\\mu)^T\\Sigma^{-1}\\mathrm d\\Sigma\\Sigma^{-1}(x_v-\\mu) - \\sum_{x_h}\\int_s p(s)([x_h,s]-\\mu)^T\\Sigma^{-1}\\mathrm d\\Sigma\\Sigma^{-1}([x_h,s]-\\mu)$$\n",
    "$$tr(\\sum_{x_v}(x_v-\\mu)^T\\Sigma^{-1}\\mathrm d\\Sigma\\Sigma^{-1}(x_v-\\mu)) = \\sum_{x_v}tr((x_v-\\mu)^T\\Sigma^{-1}\\mathrm d\\Sigma\\Sigma^{-1}(x_v-\\mu)) = \\sum_{x_v}tr(\\Sigma^{-1}(x_v-\\mu)^T(x_v-\\mu)\\Sigma^{-1}\\mathrm d\\Sigma) = tr(\\Sigma^{-1}\\sum_{x_v}(x_v-\\mu)^T(x_v-\\mu)\\Sigma^{-1}\\mathrm d\\Sigma)$$\n",
    "$$tr(\\sum_{x_h}\\int_s p(s)([x_h,s]-\\mu)^T\\Sigma^{-1}\\mathrm d\\Sigma\\Sigma^{-1}([x_h,s]-\\mu))) = tr(\\Sigma^{-1}\\sum_{x_h}\\int_s p(s)([x_h,s]-\\mu)^T([x_h,s]-\\mu))\\Sigma^{-1}\\mathrm d\\Sigma$$ \n",
    "\n",
    "$$\\mathrm dG = tr([\\Sigma^{-1}\\sum_{x_h}\\int_s p(s)([x_h,s]-\\mu)^T([x_h,s]-\\mu))\\Sigma^{-1} +\\Sigma^{-1}\\sum_{x_v}(x_v-\\mu)^T(x_v-\\mu)\\Sigma^{-1} + N\\Sigma^{-1}] \\mathrm d\\Sigma)$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial G}{\\partial \\Sigma} &= \\Sigma^{-1}\\sum_{x_h}\\int_s p(s)([x_h,s]-\\mu)^T([x_h,s]-\\mu))\\Sigma^{-1} +\\Sigma^{-1}\\sum_{x_v}(x_v-\\mu)^T(x_v-\\mu)\\Sigma^{-1} + N\\Sigma^{-1} = 0\\\\\n",
    "&\\Leftrightarrow -\\Sigma^{-1}\\sum_{x_h}\\int_s p(s)([x_h,s]-\\mu)^T([x_h,s]-\\mu)) -\\Sigma^{-1}\\sum_{x_v}(x_v-\\mu)^T(x_v-\\mu) + N = 0\\\\\n",
    "&\\Leftrightarrow \\Sigma = \\frac1N [\\sum_{x_h}\\int_s p(s)([x_h,s]-\\mu)([x_h,s]-\\mu))^T + \\sum_{x_v}(x_v-\\mu)(x_v-\\mu)^T]\n",
    "\\end{aligned}$$\n",
    "$$Var(s) = \\sigma^2_{33} + \\begin{pmatrix} \\sigma^2_{11} & \\sigma^2_{12} \\end{pmatrix}\\begin{pmatrix}\\sigma^2_{11} & \\sigma^2_{21}\\\\ \\sigma^2_{12} & \\sigma^2_{22}\\end{pmatrix}^{-1}\\begin{pmatrix} \\sigma^2_{21} \\\\ \\sigma^2_{22} \\end{pmatrix}$$\n",
    "$$\\begin{aligned}\n",
    "N\\Sigma =& \\sum_{x_h}\\int_s p(s)([x_h,s]-\\mu)([x_h,s]-\\mu))^T + \\sum_{x_v}(x_v-\\mu)(x_v-\\mu)^T\\\\\n",
    "&=\\sum_{x_h}\\int_s p(s)[x_h,s][x_h,s]^T - \\mu [x_h,s]^T - [x_h,s]\\mu^T + \\mu\\mu^T  + \\sum_{x_v}(x_v-\\mu)(x_v-\\mu)^T\\\\\n",
    "&=\\sum_{x_h}\\int_s p(s)[x_h,s][x_h,s]^T + \\sum_{x_h} \\mu\\mu^T - \\mu [x_h,E(s)]^T - [x_h,E(s)]\\mu^T + \\sum_{x_v}(x_v-\\mu)(x_v-\\mu)^T\\\\\n",
    "&=\\sum_{x_h}\\int_s p(s)\\begin{pmatrix} x_hx_h^T & x_hs\\\\sx_h^T & s^2\\end{pmatrix}+ \\sum_{x_h} \\mu\\mu^T - \\mu [x_h,E(s)]^T - [x_h,E(s)]\\mu^T + \\sum_{x_v}(x_v-\\mu)(x_v-\\mu)^T\\\\\n",
    "&=\\sum_{x_h}\\begin{pmatrix} x_hx_h^T & x_hE(s)\\\\E(s)x_h^T & E(s^2)\\end{pmatrix}+ \\sum_{x_h} \\mu\\mu^T - \\mu [x_h,E(s)]^T - [x_h,E(s)]\\mu^T + \\sum_{x_v}(x_v-\\mu)(x_v-\\mu)^T\\\\\n",
    "E(s^2) &= Var(s) + E^2(s)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum Up and Coding\n",
    "\n",
    "Now its time to sum up what to do with Python:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "E(s) &= \\mu_3 + \\begin{pmatrix} \\sigma^2_{11} & \\sigma^2_{12} \\end{pmatrix}\\\\\n",
    "Var(s) &= \\sigma^2_{33} + \\begin{pmatrix} \\sigma^2_{11} & \\sigma^2_{12} \\end{pmatrix}\\begin{pmatrix}\\sigma^2_{11} & \\sigma^2_{21}\\\\ \\sigma^2_{12} & \\sigma^2_{22}\\end{pmatrix}^{-1}\\begin{pmatrix} \\sigma^2_{21} \\\\ \\sigma^2_{22} \\end{pmatrix}\\\\\n",
    "E(s^2) &= Var(s) + E^2(s)\\\\\n",
    "\\mu &= \\frac{\\sum_{x_h}[x_h,E(s)] + \\sum_{x_v} x_v}{N}\\\\\n",
    "\\Sigma &=\\frac1N[\\sum_{x_h}\\begin{pmatrix} x_hx_h^T & x_hE(s)\\\\E(s)x_h^T & E(s^2)\\end{pmatrix}+ \\sum_{x_h} \\mu\\mu^T - \\mu [x_h,E(s)]^T - [x_h,E(s)]\\mu^T + \\sum_{x_v}(x_v-\\mu)(x_v-\\mu)^T]\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "#### Baseline: NO missing data\n",
    "\n",
    "We can easily find out that the MLE for uniform distribution can get the following results\n",
    "\n",
    "$$\\begin{cases}\n",
    "x_l = \\min(x)\\\\\n",
    "x_u = \\max(x)\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_l = [-0.4    0.055 -0.18 ]; x_u = [ 0.38  0.69  0.12]\n"
     ]
    }
   ],
   "source": [
    "y = np.array(Y)\n",
    "xl = np.min(y,axis = 1)\n",
    "xu = np.max(y, axis = 1)\n",
    "print ('x_l = {}; x_u = {}'.format(xl,xu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM method\n",
    "\n",
    "We want to maximize the likelihood function $l(\\theta)$, we set the latent parameter as $s$\n",
    "\n",
    "$$l(\\theta) = \\sum_{x_v}\\ln(P(x_v|\\theta)) + \\sum_{x_h}\\ln(P(x_h|\\theta)) = \\sum_{x_v}\\ln(P(x_v|\\theta)) + \\sum_{x_h}\\ln(\\sum_s P(x_h,s|\\theta)) \\ge \\sum_{x_v}\\ln(P(x_v|\\theta)) + \\sum_{x_h}\\sum_s q(s)(\\ln P(x_h,s|\\theta) - \\ln q(s))$$\n",
    "\n",
    "We are optimizing:\n",
    "\n",
    "$$F(q,\\theta) = \\sum_{x_v}\\ln(P(x_v|\\theta)) + \\sum_{x_h}\\sum_s q(s)(\\ln P(x_h,s|\\theta) - \\ln q(s))$$\n",
    "\n",
    "E-step: $q_{[k+1]}(s) = P(s|x_h,\\theta_{[k]})$\n",
    "\n",
    "M-step $\\theta_{[k+1]} = \\mathrm{argmax}_\\theta\\ \\  \\sum_{x_h}\\sum_sP(s|x_h,\\theta_{[k]})\\ln P(x_h,s|\\theta) + \\sum_{x_v}\\ln(P(x_v|\\theta)) = \\mathrm{argmax}_\\theta \\ \\ F(\\theta,\\theta_k)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform Ditribution\n",
    "\n",
    "We suppose that the parameters in this case should be described as $x_l = [x_{l1},x_{l2},x_{l3}],x_u = [x_{u1},x_{u2},x_{u3}]$, Then $F(\\theta,\\theta_k)$ should be described as the following\n",
    "\n",
    "$$\\begin{aligned}\n",
    "F(\\theta,\\theta_k) &= \\sum_{x_h}\\int_{x_s}\\frac{1}{x_{u3} - x_{l3}}\\ln(\\frac{1}{(x_{u1} - x_{l1})(x_{u2} - x_{l2})(x_{u3} - x_{l3})}) + \\sum_{x_h}\\ln(\\frac{1}{(x_{u1} - x_{l1})(x_{u2} - x_{l2})(x_{u3} - x_{l3})})\\\\\n",
    "F(\\theta,\\theta_k) &= \\sum_{x_h}\\ln(\\frac{1}{(x_{u1} - x_{l1})(x_{u2} - x_{l2})(x_{u3} - x_{l3})}) + \\sum_{x_h}\\ln(\\frac{1}{(x_{u1} - x_{l1})(x_{u2} - x_{l2})(x_{u3} - x_{l3})})\n",
    "\\end{aligned}$$\n",
    "\n",
    "We can easily find out that the $x_{u1} = \\max(x_{v1},x_{h1}),x_{u2} = \\max(x_{v2},x_{h2}),x_{u3} = \\max(x_{v3})$ should be the result, while $x_{l*}$ would be the min one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xl = [-0.4, 0.055, -0.18], Xu = [0.38, 0.69, 0.089]\n"
     ]
    }
   ],
   "source": [
    "xl = [min(min(*Yv[0]),min(*Yh[0])),min(min(*Yv[1]),min(*Yh[1])),min(*Yv[2])]\n",
    "xu = [max(max(*Yv[0]),max(*Yh[0])),max(min(*Yv[1]),max(*Yh[1])),max(*Yv[2])]\n",
    "print ('Xl = {}, Xu = {}'.format(xl,xu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "#### Uniform distribution:\n",
    "\n",
    "The EM algorithm (based on the expection of the latent variables) cannot predict the value that cannot appear in the original dataset, therefore, the PDF is a little bit 'smaller' than the original one:\n",
    "\n",
    "||<center>$X_u$</center>|<center>$X_l$</center>|\n",
    "|---|---|---|\n",
    "|Original|$\\begin{matrix}0.38 & 0.69 & 0.12\\end{matrix}$|$\\begin{matrix}-0.4 & 0.055 & -0.18\\end{matrix}$|\n",
    "|EM algorithm|$\\begin{matrix}0.38& 0.69& 0.089\\end{matrix}$|$\\begin{matrix}-0.4 & 0.055 & -0.18\\end{matrix}$|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
