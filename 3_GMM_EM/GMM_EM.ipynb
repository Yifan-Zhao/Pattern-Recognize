{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model and Expectation Maximization Algorithm\n",
    "\n",
    "> Weitong Zhang\n",
    "> 2015011493\n",
    ">\n",
    "> <zwt15@mails.tsinghua.edu.cn>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM and Gradient Descent\n",
    "\n",
    "Setting $\\sigma^2 = \\beta$, we get \n",
    "\n",
    "$$P(x_i|\\mu_k,\\beta_kI,x_i \\in \\omega_k) = \\frac1{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x-\\mu_k\\|_2^2}{\\beta_k})$$\n",
    "\n",
    "### Calc $\\mu$\n",
    "\n",
    "#### EM method on $\\mu$\n",
    "\n",
    "$$\\begin{aligned}z_{ik} &= Prob(x_i \\in \\omega_k | x_i,\\mu_k,\\beta_k) = \\frac{P(x_i|\\mu_k,\\beta_k,x_i \\in \\omega_k)P(\\omega_k)}{\\sum_jP(x_i|\\mu_j,\\beta_j,x_i \\in \\omega_j)P(\\omega_j)} \\\\&= \\frac{\\beta_k^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\pi_k}{\\sum_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})\\pi_j}\\end{aligned}$$\n",
    "\n",
    "We want to maximize the following function:\n",
    "\n",
    "$$\\begin{aligned}f(\\mu) &= \\sum_{i=1}^n\\sum_{k=1}^Kz_{ik}[\\ln [\\frac1{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})] + \\ln \\pi_k] \\\\&= \\sum_{i=1}^n\\sum_{k=1}^Kz_{ik}[-\\frac d2\\ln (2\\pi\\beta_k) - 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}  + \\ln \\pi_k]\\end{aligned}$$\n",
    "\n",
    "We have to notice that the $z_{ik}$ is determined by the previous step and should be a constant in this step, therefore, $\\forall i,j,k, \\frac{\\partial z_{ik}}{\\partial\\mu_j} = 0$\n",
    "\n",
    "Therefore, we get:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial f(\\mu)}{\\partial \\mu_k} &= \\frac\\partial{\\partial \\mu_k} \\sum_{i=1}^n\\sum_{k=1}^Kz_{ik}[-\\frac d2\\ln (2\\pi\\beta_k) - 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}  + \\ln \\pi_k] \\\\\n",
    "&= \\frac\\partial{\\partial \\mu_k} \\sum_{i=1}^nz_{ik}[-\\frac d2\\ln (2\\pi\\beta_k) - 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}  + \\ln \\pi_k] \\\\\n",
    "&=\\frac\\partial{\\partial \\mu_k} \\sum_{i=1}^nz_{ik}[- 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}] = 0 \\Leftrightarrow \\frac\\partial{\\partial \\mu_k} \\sum_{i=1}^nz_{ik}[- 0.5\\|x_i-\\mu_k\\|_2^2] = 0 \\\\\n",
    "&\\Rightarrow \\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^nz_{ik}x_i}{\\sum_{i=1}^nz_{ik}} = \n",
    "\\frac{\\sum_{i=1}^n\\frac{x_i\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\pi_k}{\\sum_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})\\pi_j}}{\\sum_{i=1}^n\\frac{\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\pi_k}{\\sum_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})\\pi_j}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "#### Gradient Descent method on $\\mu$\n",
    "\n",
    "$$l(\\mu) = \\sum_{i=1}^n\\ln(\\sum_{k=1}^K\\frac{\\pi_k}{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial \\mu_k} &= \\sum_{i=1}^n\\frac{\\partial}{\\partial \\mu_k}\\ln(\\sum_{k=1}^K\\frac{\\pi_k}{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}) \\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\frac{\\partial}{\\partial \\mu_k} \\pi_k\\beta_k^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{(\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})} \\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})(x_i-\\mu_k)}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}\\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}x_i - \\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}\\mu_k\n",
    "\\end{aligned}$$\n",
    "\n",
    "By setting \n",
    "\n",
    "$$\\eta_k = \\frac1{\\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}} > 0$$\n",
    "\n",
    "We can conclude that \n",
    "\n",
    "$$\\mu_k^{(t+1)} = \\mu_k + \\eta_k\\nabla l(\\mu) = \\frac{\\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}x_i}{\\sum_{i=1}^n\\frac{\\pi_k\\beta_k^{-\\frac d2 -1}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\pi_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j}}} = \\frac{\\sum_{i=1}^n\\frac{x_i\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\pi_k}{\\sum_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})\\pi_j}}{\\sum_{i=1}^n\\frac{\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\pi_k}{\\sum_j\\beta_j^{-\\frac d2}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})\\pi_j}}$$\n",
    "\n",
    "Therefore, we can prove that the Gradient Descent method and the EM algorithm is the same with calculating $\\mu$\n",
    "\n",
    "### Calc $\\beta = \\sigma^2$\n",
    "\n",
    "To begin with, let's write down the $f$ and $l$ above:\n",
    "\n",
    "$$\\begin{cases}\n",
    "l(\\beta) = \\sum_{i=1}^n\\ln(\\sum_{k=1}^K\\frac{\\pi_k}{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\\\\n",
    "f(\\beta) = \\sum_{i=1}^n\\sum_{k=1}^Kz_{ik}[-\\frac d2\\ln (2\\pi\\beta_k) - 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k}  + \\ln \\pi_k]\n",
    "\\end{cases}$$\n",
    "\n",
    "#### EM method on $\\beta = \\sigma^2$\n",
    "\n",
    "We are about to maximize the function $f(\\beta)$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\frac{\\partial f}{\\partial \\beta_k} = \\sum_{i=1}^nz_{ik}[-\\frac d2\\frac1{\\beta_k} + 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k^2}] = 0, \\beta_k \\ne 0 \\\\\n",
    "&\\Leftrightarrow \\sum_{i=1}^nz_{ik}[-\\frac d2{\\beta_k} + 0.5\\|x_i-\\mu_k\\|_2^2] = 0\\\\\n",
    "&\\Leftrightarrow \\beta_k = \\frac{\\sum_{i=1}^nz_{ik}\\|x_i-\\mu_k\\|_2^2}{d\\sum_{i=1}^nz_{ik}} =\n",
    "\\end{aligned}$$\n",
    "\n",
    "$d$ is the dimension of stochastic variable $x$, and we have to point out the $|\\Sigma|=|\\beta I| = \\beta^d$ in the PDF of Gaussian distribution\n",
    "\n",
    "#### Gradient Descent method on $\\beta$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial f}{\\partial \\beta} &= \\sum_{i=1}^n\\frac\\partial{\\partial \\beta}\\ln(\\sum_{k=1}^K\\frac{\\pi_k}{(2\\pi\\beta_k)^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})\\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\frac\\partial{\\partial \\beta} \\frac{\\pi_k}{\\beta_k^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})}{\\sum_{j=1}^K\\frac{\\pi_j}{\\beta_j^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})}\\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})(-\\frac d2\\frac{\\pi_k}{\\beta_k^{\\frac d2 +1}} + 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k^2}\\frac{\\pi_k}{\\beta_k^{\\frac d2}})}{\\sum_{j=1}^K\\frac{\\pi_j}{\\beta_j^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})}\\\\\n",
    "&=\\sum_{i=1}^n\\frac{\\frac{\\pi_k}{\\beta_k^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k})(-\\frac d2\\frac{1}{\\beta_k} + 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k^2})}{\\sum_{j=1}^K\\frac{\\pi_j}{\\beta_j^{\\frac d2}}\\exp(-0.5\\frac{\\|x_i-\\mu_j\\|_2^2}{\\beta_j})}\\\\\n",
    "&=\\sum_{i=1}^nz_{ik}(-\\frac d2\\frac{1}{\\beta_k} + 0.5\\frac{\\|x_i-\\mu_k\\|_2^2}{\\beta_k^2})\n",
    "\\end{aligned}$$\n",
    "\n",
    "Let $s_k = \\frac1{\\sum_{i=1}^nz_{ik}\\frac d2}\\beta_k^2$, therefore,\n",
    "\n",
    "$$\\beta_k^{(t+1)} = \\beta_k + s_k\\nabla l(\\beta) =\\frac{\\sum_{i=1}^nz_{ik}0.5\\|x_i-\\mu_k\\|_2^2}{\\sum_{i=1}^nz_{ik}\\frac d2} = \\frac{\\sum_{i=1}^nz_{ik}\\|x_i-\\mu_k\\|_2^2}{d\\sum_{i=1}^nz_{ik}}$$\n",
    "\n",
    "Therefore, we can conclude that the Gradient Descent method and EM algorithm is equivalence in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM for MAP Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
