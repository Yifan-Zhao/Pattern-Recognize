{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "> Weitong ZHANG, 2015011493\n",
    ">\n",
    "> zwt15@mails.tsinghua.edu.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BP for CNN network\n",
    "\n",
    "First of all, let's define a set of symbols to simplify the describtion below\n",
    "\n",
    "The input show be described as $\\mathbf A = \\begin{pmatrix} a_{11} & a_{12} & a_{13} & a_{14} \\\\ a_{21} & a_{22} & a_{23} & a_{24} \\\\ a_{31} & a_{32} & a_{33} & a_{34} \\\\ a_{41} & a_{42} & a_{43} & a_{44} \\end{pmatrix}, \\mathbf K = \\begin{pmatrix} k_{11} & k_{12} & k_{13} \\\\ k_{21} & k_{22} & k_{23} \\\\ k_{31} & k_{32} & k_{33}\\end{pmatrix}$, in this specific problems, we should define the following vector\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\mathbf A_1 = \\begin{pmatrix} a_{11} & a_{12} & a_{13} & a_{21} & a_{22} & a_{23} & a_{31} & a_{32} & a_{33} \\end{pmatrix}\\\\\n",
    "\\mathbf A_2 = \\begin{pmatrix} a_{12} & a_{13} & a_{14} & a_{22} & a_{23} & a_{24} & a_{32} & a_{33} & a_{34} \\end{pmatrix}\\\\\n",
    "\\mathbf A_3 = \\begin{pmatrix} a_{21} & a_{22} & a_{23} & a_{31} & a_{32} & a_{33} & a_{41} & a_{42} & a_{43} \\end{pmatrix}\\\\\n",
    "\\mathbf A_4 = \\begin{pmatrix} a_{22} & a_{23} & a_{24} & a_{32} & a_{33} & a_{34} & a_{42} & a_{43} & a_{44} \\end{pmatrix}\\\\\n",
    "\\mathbf A' = \\begin{pmatrix} \\mathbf A_1^\\mathrm T & \\mathbf A_2^\\mathrm T & \\mathbf A_3^\\mathrm T & \\mathbf A_4^\\mathrm T \\end{pmatrix}^\\mathrm T\\\\\n",
    "\\mathbf K' = \\begin{pmatrix} k_{11} & k_{12} & k_{13} & k_{21} & k_{22} & k_{23} & k_{31} & k_{32} & k_{33}\\end{pmatrix}^\\mathrm T\\\\\n",
    "\\mathbf E_4 = \\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix}^\\mathrm T\n",
    "\\end{cases}$$\n",
    "\n",
    "It is obvious that $\\mathbf K$ could be determined by $\\mathbf K'$, therefore, we focus on the solution on $\\mathbf K'$, hidden layer (flatten layer) should be set as $\\mathbf U_1, \\mathbf U_2$. Then we got:\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\mathbf U_1 = \\mathbf A'\\mathbf K' + \\mathbf E_4 B\\\\\n",
    "\\mathbf U_2 = \\mathbf w_1^\\mathrm T f(\\mathbf U_1) + b_1\\\\\n",
    "y = \\mathbf w_2 ^\\mathrm T f(\\mathbf U_2) + b_2\\\\\n",
    "Loss = L(f(y))\n",
    "\\end{cases}$$\n",
    "\n",
    "$f\\begin{pmatrix}x_1 & x_2 & \\cdots & x_n \\end{pmatrix}^\\mathrm T = \\begin{pmatrix}f(x_1) & f(x_2) & \\cdots & f(x_n) \\end{pmatrix}^\\mathrm T,\\frac{\\partial f(\\mathbf x)}{\\partial \\mathbf x} = diag\\begin{pmatrix} f'(x_1) & f'(x_2) & \\cdots & f'(x_n)\\end{pmatrix} = \\mathbf J_n(\\mathbf x)$\n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial \\mathbf U_1} = \\frac{\\partial f(\\mathbf U_1)}{\\partial \\mathbf U_1}\\frac{\\partial \\mathbf U_2}{\\partial f(\\mathbf U_1)}\\frac{\\partial f(\\mathbf U_2)}{\\partial \\mathbf U_2}\\frac{\\partial y}{\\partial f(\\mathbf U_2)}\\frac{\\partial L}{\\partial y} = \\mathbf J_4(\\mathbf U_1)\\mathbf w_1\\mathbf J_2(\\mathbf U_2)\\mathbf w_2f'(y)L'(f(y))$$\n",
    "\n",
    "Therefore we get:\n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial \\mathbf K'} = \\mathbf A'^\\mathrm T\\frac{\\partial Loss}{\\partial \\mathbf U_1}, \\frac{\\partial Loss}{\\partial B} = \\mathbf E_4^\\mathrm T\\frac{\\partial Loss}{\\partial \\mathbf U_1}$$\n",
    "\n",
    "$$\\mathbf K'_{t+1} = \\mathbf K'_{t} - \\lambda\\frac{\\partial Loss}{\\partial \\mathbf K'}, B_{t+1} = B_{t} - \\lambda\\frac{\\partial Loss}{\\partial B}$$\n",
    "\n",
    "where $\\lambda$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BP for RNN network\n",
    "\n",
    "### Cross-Entropy loss function\n",
    "\n",
    "In order to avoid confusion, we redefined the label $\\mathbf Y$ as $\\mathbf L$, $\\mathbf L = \\begin{pmatrix}\\mathbf L_1 & \\mathbf L_2 & \\mathbf L_3\\end{pmatrix}$\n",
    "\n",
    "Cross-Entropy: $Loss = -\\sum_{i=1}^3vec(\\mathbf L_i)^\\mathrm T\\log(\\mathbf Y_i)_{element-wise}, \\frac{\\partial Loss}{\\partial \\mathbf Y_i} = - \\frac{\\mathbf L_i}{\\mathbf Y_i}_{element-wise}$ \n",
    "\n",
    "### Activation Layers\n",
    "\n",
    "Softmax Layer: $s_i = \\frac{e^{x_i}}{\\sum_{j=1}^N{e^{x_j}}}, 1 \\le i \\le N, \\frac{\\partial s_i}{\\partial x_i} = \\frac{e^{x_i}}{\\sum_{j=1}^N{e^{x_j}}} - \\frac{e^{2x_i}}{(\\sum_{j=1}^N{e^{x_j}})^2},\\frac{\\partial s_i}{\\partial x_j} =  - \\frac{e^{x_i + x_j}}{(\\sum_{j=1}^N{e^{x_j}})^2},\\ \\frac{\\partial \\mathbf s}{\\partial \\mathbf x} = diag(\\mathbf s) - \\mathbf s \\mathbf s^\\mathrm T$\n",
    "\n",
    "Sigmoid function: $y_i = \\frac1{1 + \\exp(-x_i)}, \\frac{\\partial \\mathbf y}{\\partial \\mathbf x} = diag(\\mathbf y \\times_{element}(1-\\mathbf y))$, where $diag(\\mathbf x) = diag(x_1,x_2,\\cdots,x_n)$\n",
    "\n",
    "For the furture convenience, let's set $\\mathbf J_{sig}(\\mathbf y) = diag(\\mathbf y \\times_{element}(1-\\mathbf y)), \\mathbf J_{soft}(\\mathbf y) = diag(\\mathbf y) - \\mathbf y \\mathbf y^\\mathrm T$\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial \\mathbf V} = \\sum_{i=1}^3\\frac{\\partial \\mathbf Y_i}{\\partial \\mathbf V}\\frac{\\partial Loss}{\\partial \\mathbf Y_i} = \\sum_{i=1}^3\\frac{\\partial \\mathbf V\\mathbf H_i}{\\partial \\mathbf V}\\mathbf J_{soft}(\\mathbf Y_i)\\frac{\\partial Loss}{\\partial \\mathbf Y_i} = \\sum_{i=1}^3\\mathbf J_{soft}(\\mathbf Y_i)\\frac{\\partial Loss}{\\partial \\mathbf Y_i}\\mathbf H_i^\\mathrm T$$\n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial \\mathbf H_i} = \\sum_{i=1}^3\\frac{\\partial \\mathbf Y_i}{\\partial \\mathbf H_i}\\frac{\\partial Loss}{\\partial \\mathbf Y_i} = \\sum_{i=1}^3\\frac{\\partial \\mathbf V\\mathbf H_i}{\\partial \\mathbf H_i}\\mathbf J_{soft}(\\mathbf Y_i)\\frac{\\partial Loss}{\\partial \\mathbf Y_i} = \\sum_{i=1}^3\\mathbf V^\\mathrm T\\mathbf J_{soft}(\\mathbf Y_i)\\frac{\\partial Loss}{\\partial \\mathbf Y_i}$$\n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial \\mathbf U} = \\sum_{i=1}^3\\frac{\\partial \\mathbf H_i}{\\partial \\mathbf U}\\frac{\\partial Loss}{\\partial \\mathbf H_i} = \\sum_{i=1}^3\\mathbf J_{sig}(\\mathbf H_i)\\frac{\\partial Loss}{\\partial \\mathbf H_i}\\mathbf X_i^\\mathrm T$$\n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial \\mathbf W} = \\sum_{i=1}^3\\frac{\\partial \\mathbf H_i}{\\partial \\mathbf W}\\frac{\\partial Loss}{\\partial \\mathbf H_i} = \\sum_{i=1}^3\\mathbf J_{sig}(\\mathbf H_i)\\frac{\\partial Loss}{\\partial \\mathbf H_i}\\mathbf H_{i-1}^\\mathrm T, \\mathbf H_0 = \\mathbf 0$$\n",
    "\n",
    "In all we get the BP algorithm:\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\mathbf W_{t+1} = \\mathbf W_t - \\mathrm {lr}\\ \\frac{\\partial Loss}{\\partial \\mathbf W}\\\\\n",
    "\\mathbf U_{t+1} = \\mathbf U_t - \\mathrm {lr}\\ \\frac{\\partial Loss}{\\partial \\mathbf U}\\\\\n",
    "\\mathbf V_{t+1} = \\mathbf V_t - \\mathrm {lr}\\ \\frac{\\partial Loss}{\\partial \\mathbf V}\\\n",
    "\\end{cases}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
